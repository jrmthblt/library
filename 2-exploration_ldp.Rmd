---
title: "Exploration Livre de Poche"
author: "Jérôme THIBAULT"
date: "25/04/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
Ce notebook explore l'ensemble des résumés de livres scrapés depuis le site du Livre de Poche, toutes collections confondues. Il se décompose de la manière suivante :
- import et description succincte du jeu de données
- nettoyage du texte
- analyse descriptive
- classification non supervisée

Ces différentes étapes doivent permettre de mieux appréhender le jeu de données, avant de s'engager dans une classification supervisée.


# Déroulement de la procédure

## Chargement des packages
On commence par charger les packages nécessaires :
- tm, stringi, stringr, SnowballC : nettoyage de texte (normalisation, regexp, unicode, stemming)
- tidyverse (dont dplyr et ggplot2) : librairies-outils plus modernes, proposées par Hadley Wickham (notamment grammaire proche du SQL)
- wordcloud, RColorBrewer : générateur de nuages de mots, palettes de couleurs

```{r chargement des packages}
# install.packages("tm")
# install.packages("SnowballC")
# install.packages("wordcloud")
# install.packages("RColorBrewer")
library(tidyverse)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
```


## Import des données
On importe ensuite les données via readr (tidyverse).
On en profite pour supprimer les colonnes inutiles :

```{r import csv}
dfBookSummaries <- read_csv("dfAllBooksSummaries2405.csv", col_types = cols(X1 = col_skip(), X2 = col_skip(), text = col_character(), collection = col_factor(levels = c("Classiques", "Documents", "Imaginaire", "Littérature", "Policier", "Thriller"))))
```


## Description succincte des données
On résume le jeu de données importé : combien de collections et de résumés contient-il ?

```{r résumé}
summary(dfBookSummaries)
```

On dénombre 5096 résumés, répartis dans 6 collections. La collection littérature est sur-exprimée :

```{r résumé graphique}
library(ggplot2)
ggplot(data = dfBookSummaries) + geom_bar(mapping = aes(x = collection, colour = collection))
```

En effet cette collection représente plus de 40% du jeu de données :

```{r résumé fréquence collection}
plot(table(dfBookSummaries$collection) / sum(table(dfBookSummaries$collection)))
```


## Recherche de doublons
On s'intéresse maintenant à la présence éventuelle de doublons. En effet on ne peut pas préjuger de la qualité intrinsèque du catalogue de la maison d'édition. On va donc compter les doublons de résumés de livres :

```{r recherche de doublons}
dfBookSummaries %>% count() - dfBookSummaries %>% select(text) %>% distinct() %>% count()
```

La requête a détecté 53 doublons. Apparaissent-ils entre plusieurs collections, auquel cas ces dernières ne sont peut-être pas pures ? Ou au contraire les doublons d'un même résumé restent-ils associés à la même collection ?

```{r doublons intra inter collection}
dfBookSummaries %>% group_by(text) %>% summarise(nbDoublons = n(), nbCollections = n_distinct(collection)) %>% filter(nbDoublons > 1 | nbCollections > 1)
```

Seuls les résumés "vides" sont dispersés sur les 6 collections. Les autres résumés représentent effectivement des doublons, mais restent dans leur collection. Les collections du Livre de Poche sont donc "pures".


## Suppression des doublons et des résumés vides
On veut supprimer les doublons de résumés renseignés (on conservera donc le premier par exemple).

Pour supprimer tous les résumés vides, on opère un filter sur le dataframe.

```{r suppression doublons}
dfBookSummaries <- dfBookSummaries %>% distinct()

emptySummary <- dfBookSummaries %>% group_by(text) %>% summarise(nbDoublons = n(), nbCollections = n_distinct(collection)) %>% filter(nbDoublons > 1 | nbCollections > 1) %>% select(text)

keepOnlyLetters <- function (str) {
  result <- str
  result <- stri_replace_all_regex(str, "\\p{C}+", "")
  result <- stri_replace_all_regex(str, "\\p{Z}+", "")
  return(result)
}

dfBookSummaries <- dfBookSummaries %>% filter(keepOnlyLetters(text) != keepOnlyLetters(emptySummary))
```

On a donc supprimé 54 lignes du jeu de données (6 résumés vides et 48 doublons de résumés).


## Nettoyage du corpus de documents



```{r corpus et dtm}
docs <- Corpus(VectorSource(dfBookSummaries$text))

inspect(docs[1])

# Suppression des accents : OK
library(stringi)
library(stringr)

nfdRemoveAccents <- function(x) {
  nfdX <- stri_trans_nfd(x)
  return(stri_replace_all_regex(nfdX, "\\p{Mn}+", ""))
}
docs <- tm_map(docs, content_transformer(nfdRemoveAccents))

# Convertir le texte en minuscule : OK
docs <- tm_map(docs, content_transformer(tolower))

# Remplacer toute la ponctuation par des espaces : OK
# FIXME : attention les stopwords pourraient ne plus fonctionner (l' remplacé par l)
removePonctuation <- function(x) {
  return(stri_replace_all_regex(x, "\\p{P}+", " "))
}
docs <- tm_map(docs, content_transformer(removePonctuation))

# Supprimer le texte en gras récupéré par erreur par le scraper (et difficile à automatiser)
# docs <- tm_map(docs, toSpace, "collection dirigée par gérard klein")

# Supprimer les nombres
docs <- tm_map(docs, removeNumbers)
# Supprimer une liste de mots non supportés par le dictionnaire de stop words fr
docs <- tm_map(docs, removeWords, c("plus", "les", "d", "l", "c", "d", "resume", "science-fiction", "ou", "plus", "tome", "serie", "roman", "romans", "collection", "livre", "gerard", "dirigee", "prix", "king", "auteur", "stephen"))

# Supprimer les mots outils français
 # FIXME : corriger les problèmes d'accent
# mySW = read.csv("jltStopwords.txt", sep = "", stringsAsFactors = FALSE, header = FALSE)
# docs <- tm_map(docs, removeWords, mySW$V1)

docs <- tm_map(docs, removeWords, stopwords("french"))
# Supprimer les ponctuations
docs <- tm_map(docs, removePunctuation)
# Supprimer les espaces vides supplémentaires
docs <- tm_map(docs, stripWhitespace)


# FIXME : Text stemming. Donne de drôles de résultats !!
# docs <- tm_map(docs, stemDocument)


# TODO : voir s'il faut éliminer les mots de moins de 2/3 lettres


dtm <- DocumentTermMatrix(docs)
dtm
# <<DocumentTermMatrix (documents: 5048, terms: 39573)>>
# Non-/sparse entries: 347222/199417282
# Sparsity           : 100%
# Maximal term length: 30
# Weighting          : term frequency (tf)

# distribution de la longueur d'un terme, distribution du nombre de mots par document
dtmMatrix <- as.matrix(dtm) # renvoie pour chaque document le nombre d'occurrences de chaque terme y figurant

rowSums <- rowSums(dtmMatrix) # index des documents en ligne et termes en colonne
dtmMatrix <- dtmMatrix[which(rowSums >= 10),]
dtmTibble <- as_tibble(dtmMatrix) # 18 documents supprimés car contenant moins de 10 termes (uniques ou répétés)
rowSums <- rowSums(dtmMatrix) # index des documents en ligne et termes en colonne

# FIXME : Erreur : ggplot2 doesn't know how to deal with data of class numeric
# ggplot(data = rowSums) + geom_bar(mapping = aes(x = dfBookSummaries$genre, colour = as.factor(dfBookSummaries$genre)))

# Distribution du nombre de mots par genre
summary(rowSums)

plot(rowSums, col = as.factor(dfBookSummaries$genre))
abline(h=median(rowSums), col = "green")
hist(rowSums, breaks = 100)
boxplot(rowSums~dfBookSummaries$genre, col = as.factor(dfBookSummaries$genre))

dtmMatrix <- NULL

# TODO : Est-ce que dans le résumé, on trouve le nom du genre ? Tf-IDF donnera moins d'importance à "Science" s'il est très répandu dans tout le corpus.


```


## Description plus complète après nettoyage

```{r contrôle descriptif}
dtm <- DocumentTermMatrix(docs)
dfDTM <- as.data.frame(as.matrix(dtm))

library(wordcloud2)

categories <- levels(as.factor(dfBookSummaries$genre))
for (i in 1:length(categories)){
  m <- dfDTM[dfBookSummaries$genre == categories[i],]
  wordsFreq <- sort(colSums(m),decreasing = TRUE)
  wordsFreq <- data.frame(word = names(wordsFreq), Freq = as.vector(wordsFreq)) # mise au format pour wordcloud2
  print(wordcloud2(data = wordsFreq[1:500,],minSize = 1, size = 3))
}


# TODO : récupérer les couvertures ?
# TODO : topic modeling pour adresser les synonymes ?



minfreq <- findFreqTerms(dtm, 10)
dtm <- DocumentTermMatrix(docs, control=list(dictionary = minfreq, weight=weightTfIdf))
dtmCleanMatrix <- as.data.frame(as.matrix(dtm))

# TODO : on pourrait encore supprimer quelques mots (en regardant les noms des colonnes de la DTM). Voir s'ils sont associés souvent à d'autres mots ? Type saga la plus vendue
# Normalement on peut faire cette manip, car n'influence pas le TF-IDF :
# dtmCleanMatrix$maigret <- NULL
# dtmCleanMatrix$nouvelle <- NULL


library(wordcloud2)

categories <- levels(as.factor(dfBookSummaries$genre))
for (i in 1:length(categories)){
  m <- dtmCleanMatrix[dfBookSummaries$genre == categories[i],]
  wordsFreq <- sort(colSums(m),decreasing = TRUE)
  wordsFreq <- data.frame(word = names(wordsFreq), Freq = as.vector(wordsFreq)) # mise au format pour wordcloud2
  print(wordcloud2(data = wordsFreq[1:500,],minSize = 1, size = 3))
}


# Après nettoyage, on descend de plus de 100 mots par résumé à moins de 40
rowSums <- rowSums(dtmCleanMatrix)
plot(rowSums, col = as.factor(dfBookSummaries$genre))
abline(h=median(rowSums), col = "green")
hist(rowSums, breaks = 100)


# Similarité entre mots
# TODO : findAssocs(dtm, terms = "monde", corlimit = 0.3)

# TODO : Word2Vec : mais pour faire quoi exactement dans notre cas ?

# TODO : Classification non supervisée : SKmeans / CAH / LDA
```


## Classification non supervisée LDA (Topic Modeling)

```{r topic modeling}
rowTotals <- apply(dtmCleanMatrix , 1, sum) # calcule la somme des termes dans chaque document
dtmCleanMatrix   <- dtmCleanMatrix[rowTotals> 0, ] # retire les documents vides, il peut arriver après nettoyage de se retrouver avec des documents vides, par exemple si on travaille sur des documents très courts type tweet

# install.packages("topicmodels")
library(topicmodels)
# Paramètres du Gibbs sampling
burnin <- 500
iter <- 500
thin <- 100
seed <- list(2003,5,63,100001,765) # arbitraire
nstart <- 5
best <- TRUE

# Nombre de thèmes
k <- 2 # arbitraire à ce stade, nécessiterait d'être optimisé

# Estimation de la LDA
ldaOut <-LDA(dtmCleanMatrix,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))


ldaOut.topics <- as.matrix(topics(ldaOut))
table(ldaOut.topics)

ldaOut.terms <- as.matrix(terms(ldaOut,10))
ldaOut.terms

topicProbabilities <- as.data.frame(ldaOut@gamma)
names(topicProbabilities) <- apply(data.frame(ldaOut.terms),2,function(x) paste(x,collapse=' '))
topicProbabilities[1:100,]  #pour les dix premiers documents arbitrairement, on voit que la plupart du temps un topic est prépondérant.

# TODO : tester la précision des catégories vs. les Y de sortie du premier dataframe (for JL : "accuracy").
# TODO : kmeans et CAH (sur matrice nettoyée)
```

