---
title: "Exploration Livre de Poche"
author: "Jérôme THIBAULT"
date: "25/04/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Chargement des packages

```{r chargement des packages}
# Installer
# install.packages("tm")  # pour le text mining
# install.packages("SnowballC") # pour le text stemming
# install.packages("wordcloud") # générateur de word-cloud
# install.packages("RColorBrewer") # Palettes de couleurs
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
```


## Import et première description du jeu de données

```{r description sommaire}
dfBookSummaries <- read_csv("dfAllBooksSummaries2405.csv")
dfBookSummaries$X1 <- NULL
dfBookSummaries$X1_1 <- NULL

library(ggplot2)
table(dfBookSummaries$genre)
table(dfBookSummaries$genre) / sum(table(dfBookSummaries$genre))
ggplot(data = dfBookSummaries) + geom_bar(mapping = aes(x = genre, colour = genre))

# Compter le nombre de doublons
length(which(duplicated(dfBookSummaries$text) == TRUE))
dfBookSummaries[which(duplicated(dfBookSummaries$text)),]

library(dplyr)


# 48 doublons de résumés trouvés, intra-collection (bcp de résumés vides car livres pas encore parus)
# On les supprime
dfBookSummaries <- dfBookSummaries %>% distinct()

# 12 résumés presque vides, à supprimer
dfBookSummaries <- dfBookSummaries[which(rowSums>=10),]

# On mélange les documents
dfBookSummaries <- dfBookSummaries[sample(nrow(dfBookSummaries)),]

docs <- Corpus(VectorSource(dfBookSummaries$text))
dtm <- DocumentTermMatrix(docs)
inspect(docs[1])
dtm
# <<DocumentTermMatrix (documents: 5048, terms: 48614)>>
# Non-/sparse entries: 436450/244967022
# Sparsity           : 100%
# Maximal term length: 35
# Weighting          : term frequency (tf)

# distribution de la longueur d'un terme, distribution du nombre de mots par document
dtmMatrix <- as.matrix(dtm) # renvoie pour chaque document/terme, le nombre de fois
rowSums <- rowSums(dtmMatrix)

# FIXME : Erreur : ggplot2 doesn't know how to deal with data of class numeric
# ggplot(data = rowSums) + geom_bar(mapping = aes(x = dfBookSummaries$genre, colour = as.factor(dfBookSummaries$genre)))


# Distribution du nombre de mots par genre
summary(rowSums)
tapply(rowSums, dfBookSummaries$genre, min)
tapply(rowSums, dfBookSummaries$genre, mean)
tapply(rowSums, dfBookSummaries$genre, max)
tapply(rowSums, dfBookSummaries$genre, sd)

plot(rowSums, col = as.factor(dfBookSummaries$genre))
abline(h=median(rowSums), col = "green")
hist(rowSums, breaks = 100)
boxplot(rowSums~dfBookSummaries$genre, col = as.factor(dfBookSummaries$genre))

dtmMatrix <- NULL

# TODO : Est-ce que dans le résumé, on trouve le nom du genre ? Tf-IDF donnera moins d'importance à "Science" s'il est très répandu dans tout le corpus.


```


## Nettoyage des données

```{r nettoyage}

# Convertir le texte en minuscule
docs <- tm_map(docs, content_transformer(tolower))
# Remplacer les points par des espaces
toSpace <- content_transformer(function (x , pattern) { gsub(pattern, " ", x)})
docs <- tm_map(docs, toSpace, "\\.")
# Remplacer les apostrophes par des espaces
docs <- tm_map(docs, toSpace, "'")
docs <- tm_map(docs, toSpace, "’")
docs <- tm_map(docs, toSpace, "«")
docs <- tm_map(docs, toSpace, "»")

toA <- content_transformer(function (x , pattern) { gsub(pattern, "a", x)})
docs <- tm_map(docs, toA, "à")
docs <- tm_map(docs, toA, "á")

toE <- content_transformer(function (x , pattern) { gsub(pattern, "e", x)})
docs <- tm_map(docs, toE, "é")
docs <- tm_map(docs, toE, "è")
docs <- tm_map(docs, toE, "ê")
docs <- tm_map(docs, toE, "ë")

toI <- content_transformer(function (x , pattern) { gsub(pattern, "i", x)})
docs <- tm_map(docs, toE, "ï")
docs <- tm_map(docs, toE, "î")

# Supprimer le texte en gras récupéré par erreur par le scraper (et difficile à automatiser)
docs <- tm_map(docs, toSpace, "collection dirigée par gérard klein")

# Supprimer les nombres
docs <- tm_map(docs, removeNumbers)
# Supprimer une liste de mots non supportés par le dictionnaire de stop words fr
docs <- tm_map(docs, removeWords, c("plus", "les", "...", "d'", "l'", "resume", "science-fiction", "où", "plus", "tome", "serie", "roman", "romans", "collection", "livre", "gerard", "dirigee", "prix", "king", "auteur", "stephen"))
# Supprimer les mots outils français
mySW = read.csv("jltStopwords.txt", sep = "", stringsAsFactors = FALSE, header = FALSE) # TODO : corriger les problèmes d'accent
docs <- tm_map(docs, removeWords, mySW$V1)
docs <- tm_map(docs, removeWords, stopwords("french"))
# Supprimer les ponctuations
docs <- tm_map(docs, removePunctuation)
# Supprimer les espaces vides supplémentaires
docs <- tm_map(docs, stripWhitespace)


# FIXME : Text stemming. Donne de drôles de résultats !!
# docs <- tm_map(docs, stemDocument)

# Suppression des accents
library(stringi)
library(stringr)
# accent <- function(x) stri_trans_general(x, "UTF8-ASCII") # cela signifie qu'on remplace un caractère encodé en Latin1 par son équivalent le plus proche en ASCII, il n'y a par exemple pas de caractères accentués en ASCII
# docs <- tm_map(docs, content_transformer(accent))

# TODO : voir s'il faut éliminer les mots de moins de 3 lettres


# On vectorise le corpus après nettoyage
dtm <- DocumentTermMatrix(docs)
```


## Description plus complète après nettoyage

```{r contrôle descriptif}
# TODO : récupérer les couvertures ?
# TODO : topic modeling pour adresser les synonymes ?



minfreq <- findFreqTerms(dtm, 10)
dtm <- DocumentTermMatrix(docs, control=list(dictionary = minfreq, weight=weightTfIdf))
dtmCleanMatrix <- as.data.frame(as.matrix(dtm))
# Normalement on peut faire cette manip, car n'influence pas le TF-IDF :
dtmCleanMatrix$maigret <- NULL
dtmCleanMatrix$nouvelle <- NULL


library(wordcloud2)

categories <- levels(as.factor(dfBookSummaries$genre))
for (i in 1:length(categories)){
  m <- dtmCleanMatrix[dfBookSummaries$genre == categories[i],]
  wordsFreq <- sort(colSums(m),decreasing = TRUE)
  wordsFreq <- data.frame(word = names(wordsFreq), Freq = as.vector(wordsFreq)) # mise au format pour wordcloud2
  print(wordcloud2(data = wordsFreq[1:500,],minSize = 1, size = 3))
}


# Après nettoyage, on descend de plus de 100 mots par résumé à moins de 40
rowSums <- rowSums(dtmCleanMatrix)
plot(rowSums, col = as.factor(dfBookSummaries$genre))
abline(h=median(rowSums), col = "green")
hist(rowSums, breaks = 100)
```


## Classification non supervisée LDA (Topic Modeling)

```{r topic modeling}
rowTotals <- apply(dtmCleanMatrix , 1, sum) # calcule la somme des termes dans chaque document
dtmCleanMatrix   <- dtmCleanMatrix[rowTotals> 0, ] # retire les documents vides, il peut arriver après nettoyage de se retrouver avec des documents vides, par exemple si on travaille sur des documents très courts type tweet

# install.packages("topicmodels")
library(topicmodels)
# Paramètres du Gibbs sampling
burnin <- 500
iter <- 500
thin <- 100
seed <- list(2003,5,63,100001,765) # arbitraire
nstart <- 5
best <- TRUE

# Nombre de thèmes
k <- 2 # arbitraire à ce stade, nécessiterait d'être optimisé

# Estimation de la LDA
ldaOut <-LDA(dtmCleanMatrix,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))


ldaOut.topics <- as.matrix(topics(ldaOut))
table(ldaOut.topics)

ldaOut.terms <- as.matrix(terms(ldaOut,10))
ldaOut.terms

topicProbabilities <- as.data.frame(ldaOut@gamma)
names(topicProbabilities) <- apply(data.frame(ldaOut.terms),2,function(x) paste(x,collapse=' '))
topicProbabilities[1:100,]  #pour les dix premiers documents arbitrairement, on voit que la plupart du temps un topic est prépondérant.

# TODO : tester la précision des catégories vs. les Y de sortie du premier dataframe (for JL : "accuracy").
# TODO : kmeans et CAH (sur matrice nettoyée)
```

