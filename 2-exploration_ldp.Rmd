---
title: "Exploration Livre de Poche"
author: "Jérôme THIBAULT"
date: "25/04/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
Ce notebook explore l'ensemble des résumés de livres scrapés depuis le site du Livre de Poche, toutes collections confondues. Il se décompose de la manière suivante :
- import et description succincte du jeu de données
- nettoyage du texte
- analyse descriptive
- classification non supervisée

Ces différentes étapes doivent permettre de mieux appréhender le jeu de données, avant de s'engager dans une classification supervisée.


## Chargement des packages
On commence par charger les packages nécessaires :
- tm, stringi, stringr, SnowballC : nettoyage de texte (normalisation, regexp, unicode, stemming)
- tidyverse (dont dplyr et ggplot2) : librairies-outils plus modernes, proposées par Hadley Wickham (notamment grammaire proche du SQL)
- wordcloud, RColorBrewer : générateur de nuages de mots, palettes de couleurs

```{r chargement des packages}
# install.packages("tm")
# install.packages("SnowballC")
# install.packages("wordcloud")
# install.packages("RColorBrewer")
# install.packages("readr")
# install.packages("tidyverse")
library(tidyverse)
library(tm)
library(stringi)
library(stringr)
library(SnowballC)
library(wordcloud)
library(wordcloud2)
library(RColorBrewer)
library(readr)
```


## Import des données
On importe ensuite les données via readr (tidyverse).
On en profite pour supprimer les colonnes inutiles :

```{r import csv}

dfBookSummaries <- read_csv("dfAllBooksSummaries2405.csv", col_types = cols(X1 = col_skip(), X2 = col_skip(), text = col_character(), collection = col_factor(levels = c("Classiques", "Documents", "Imaginaire", "Littérature", "Policier", "Thriller"))))
```


## Description succincte des données
On résume le jeu de données importé : combien de collections et de résumés contient-il ?

```{r résumé}
summary(dfBookSummaries)
```

On dénombre 5096 résumés, répartis dans 6 collections. La collection littérature est sur-exprimée :

```{r résumé graphique}
library(ggplot2)
ggplot(data = dfBookSummaries) + geom_bar(mapping = aes(x = collection, fill = collection)) + scale_fill_manual(values = palette()) + ylab("nombre de livres")
```

En effet cette collection représente plus de 40% du jeu de données :

```{r résumé fréquence collection}
plot(100*table(dfBookSummaries$collection) / sum(table(dfBookSummaries$collection)), col = palette(), ylab="pourcentage de livres de la collection")
```


## Recherche de doublons
On s'intéresse maintenant à la présence éventuelle de doublons. En effet on ne peut pas préjuger de la qualité intrinsèque du catalogue de la maison d'édition. On va donc compter les doublons de résumés de livres :

```{r recherche de doublons}
dfBookSummaries %>% count() - dfBookSummaries %>% select(text) %>% distinct() %>% count()
```

La requête a détecté 53 doublons. Apparaissent-ils entre plusieurs collections, auquel cas ces dernières ne sont peut-être pas pures ? Ou au contraire les doublons d'un même résumé restent-ils associés à la même collection ?

```{r doublons intra inter collection}
dfBookSummaries %>% group_by(text) %>% summarise(nbDoublons = n(), nbCollections = n_distinct(collection)) %>% filter(nbDoublons > 1 | nbCollections > 1)
```

Seuls les résumés "vides" sont dispersés sur les 6 collections. Les autres résumés représentent effectivement des doublons, mais restent dans leur collection. Les collections du Livre de Poche sont donc "pures".


## Suppression des doublons et des résumés vides
On veut supprimer les doublons de résumés renseignés (on conservera donc le premier par exemple).

Pour supprimer tous les résumés vides, on opère un filter sur le dataframe.

```{r suppression doublons}
dfBookSummaries <- dfBookSummaries %>% distinct()

emptySummary <- dfBookSummaries %>% group_by(text) %>% summarise(nbDoublons = n(), nbCollections = n_distinct(collection)) %>% filter(nbDoublons > 1 | nbCollections > 1) %>% select(text)

keepOnlyLetters <- function (str) {
  result <- stri_replace_all_regex(str, "\\p{C}+", "")
  result <- stri_replace_all_regex(result, "\\p{Z}+", "")
  return(result)
}

dfBookSummaries <- dfBookSummaries %>% filter(keepOnlyLetters(text) != keepOnlyLetters(emptySummary))
```

On a donc supprimé 54 lignes du jeu de données (6 résumés vides et 48 doublons de résumés).


Les précédents modèles ont montré que les frontières de la collection Littérature sont très floues :
```{r test suppression collection littérature}
dfBookSummaries <- dfBookSummaries %>% filter(collection != "Littérature")
```



## Nettoyage du corpus de documents
Maintenant que nous avons supprimé les doublons et validé le jeu de données, nous pouvons normaliser le texte pour pouvoir ensuite l'analyser.

On se base sur le package tm, qui automatise une partie significative des traitements, et on le complète par le package stringi stringr qui gère très bien l'unicode et les regexps.

On créé d'abord le corpus :

```{r corpus et dtm}
docs <- Corpus(VectorSource(dfBookSummaries$text))
DocumentTermMatrix(docs)
inspect(docs[1])
```

Sans traitement notre DTM intègre 48614 termes (autant qu'un dictionnaire !). Sa sparsity (nombre de colonnes à 0) approche donc les 100% puisque nous disposons uniquement de 5042 documents.

Comme on peut le constater, le premier résumé contient un grand nombre de mots-outils (le la les). Le texte est aussi parsemé d'accents, de ponctuations, de caractères de fin de ligne, de majuscules, d'espaces...

On va donc retirer toutes ces exceptions pour que notre future matrice se compose de mots normalisés. On réduira ainsi la dimension et l'analyse en sera forcément plus pertinente.

On supprime d'abord les accents, car notre corpus est en français. Le texte étant encodé en UTF-8, nous allons exploiter une possibilité de l'Unicode pour supprimer les accents : la décomposition NFD. Celle-ci permet de transformer chaque caractère Unicode dans sa forme canonique, et de séparer les accents des lettres. Ensuite nous appliquons une regexp qui supprime spécifiquement les accents encodés UTF-8 de cette manière : 

```{r suppression accents}
nfdRemoveAccents <- function(x) {
  nfdX <- stri_trans_nfd(x)
  return(stri_replace_all_regex(nfdX, "\\p{Mn}+", ""))
}

docs <- tm_map(docs, content_transformer(nfdRemoveAccents))
inspect(docs[1])
```

Les accents ont tous disparu. On poursuit avec la normalisation de la casse via la fonction de tm :

```{r passage en minuscule}
docs <- tm_map(docs, content_transformer(tolower))
inspect(docs[1])
```

On retire désormais la ponctuation et les caractères invisibles (dont les retours à la ligne, mais PAS encore les espaces). Pour ce faire, on recoure à plusieurs regexps sur les classes de caractères UTF-8 associées :

```{r suppression ponctuation}
removePonctuation <- function(x) {
  result <- x
  result <- stri_replace_all_regex(result, "\\p{Zl}+", " ")
  result <- stri_replace_all_regex(result, "\\p{Zp}+", " ")
  result <- stri_replace_all_regex(result, "\\p{S}+", " ")
  result <- stri_replace_all_regex(result, "\\p{N}+", " ")
  result <- stri_replace_all_regex(result, "\\p{C}+", " ")
  result <- stri_replace_all_regex(result, "\\p{P}+", " ") 
  return(result)
}
docs <- tm_map(docs, content_transformer(removePonctuation))
inspect(docs[1])
```

Toutes les classes de caractère parasites sont désormais retirées, excepté les espaces. Pour les nombres, on aurait pu utiliser la fonction removeNumbers de tm, mais la regexp fonctionne aussi bien.

On s'attaque à présent à la suppression des stopwords, qui n'apportent aucune signification au texte et augmente la dimension inutilement. Le package tm propose une liste pour le français, que nous allons compléter par la suite.

```{r suppression des stopwords}
stopwordsFr <- unlist(lapply(list(stopwords("french")), nfdRemoveAccents))

# Stopwords de Jean-Luc
mySW = read.csv("jltStopwords_UTF8.txt", sep = "", stringsAsFactors = FALSE, header = FALSE, fileEncoding = "UTF-8")
#on élimine les accents du fichier jltStopwords avec la fonction nfdRemoveAccents
mySW$V1 <- unlist(lapply(list(mySW$V1), nfdRemoveAccents))

# Dictionnaire mot à retirer - Arnaud + ceux ci-dessous (peu importe si quelques doublons)
stopwordsFrCustom <- union(mySW$V1, c('a','h','lundi','mardi','mercredi','jeudi','vendredi','samedi','dimanche',
                 'oui','non','apres','selon','comme','alors','tout','tous','faire','depuis','encore',
                 'peut','doit','mieux','un','deux','trois','quatre','cinq','six','sept','huit','neuf','dix',
                 'tant','ainsi','livre','oeuvre','aussi','fait','entre','plus','moins','toute','donc',
                 'toutes','auteur','bien','roman','comment','petit','petite','grand','grande','ceux',
                 'etc','annee','aujourd','hui','tres','seul','seule','autre','autres','celle','donc','dont',
                 'donne','sous','sur','jusqu','quelqu','nombreux','propose','part','parti','partir','jamais',
                 'car','grands','grandes','question','fois','france','travers','jour','avant','apres','il','elle',
                 'ils','elles','tel','tels','telle','telles','quelque','quelques','toujours','souvent','chez',
                 'celui','chaque','mis','mise','texte','moindre','peu','ouvrage','rien','pourtant','texte',
                 'the','certains','certaines','lecteur','vers','parfois','grace','aime','aimer','enfin',
                 'chacun','chacune','pourquoi','propos','plusieurs','avoir','etre','contre','mais','faut',
                 'puis','notre','votre','seule','seules','seulement','note','noter','edition','les','d',
                 'l','c','resume','ou','tome','serie','roman','romans','collection',
                 'livre', 'dirigee','prix','auteur','histoire','etait','meme',
                 'recueil','quinze', 'science-fiction'))

stopwords <- union(stopwordsFr, stopwordsFrCustom)

# FIXME : peut-être faut-il conserver le nom des auteurs qui apparaissent dans les résumés ? On remet 'gerard', 'king', 'stephen', 'hugo'

docs <- tm_map(docs, removeWords, stopwords)
inspect(docs[1])

```

TODO : On veut pouvoir s'assurer que la grande majorité des stopwords a été supprimée. On va donc extraire les mots les plus fréquents dans le corpus et retirer ceux qui sont des mots-outils :

```{r termes fréquents}
dtmTf <- DocumentTermMatrix(docs, control=list(weighting=weightTf))
dtmTf_m <- as.matrix(DocumentTermMatrix(docs))
v <- sort(colSums(dtmTf_m),decreasing=TRUE)
TopWords <- data.frame(word = names(v),freq = v)
head(TopWords, 50)

# Puis à la main on enlève ceux qui paraissent "creux" dans notre contexte
docs <- tm_map(docs, removeWords, c("resume", "roman", "ans", "histoire", "livre", "faire", "auteur", "nouvelle"))

# puis on vérifie
dtmTf_m <- as.matrix(DocumentTermMatrix(docs))
v <- sort(colSums(dtmTf_m),decreasing=TRUE)
TopWords <- data.frame(word = names(v),freq = v)
head(TopWords, 50)
```

On retire les mots creux : résumé, roman, histoire, livre, auteur, ...


La liste des mots les plus/moins fréquents dans le corpus peut aussi être donnée selon l'approche suivante : 
```{r top/bottom}
#Terms frequency statistics
freqr <- slam::col_sums(dtmTf)
#length should be total number of terms
length(freqr)
#length should be total number of terms
ord <- order(freqr,decreasing=TRUE)
#inspect most frequently occurring terms
freqr[head(ord,50L)]
#inspect least frequently occurring terms
freqr[tail(ord,50L)]
#View(freqr)


# dtm.tfidf <- DocumentTermMatrix(documents2, control=list(bounds = list(global = c(2,520)), weighting = function(x) weightTfIdf(x, normalize = TRUE)))
# dtm.tf <- DocumentTermMatrix(documents2, control=list(bounds = list(global = c(2,520)), weighting = function(x) weightTf(x)))

```


### Racinisation
Après suppression des stopwords, on remarque des mots proches les uns des autres : homme/hommes, femme/femmes, ...

On va raciniser le texte pour résoudre ce problème :

```{r racinisation}
# nonStemmedDocs <- docs
# docs <- tm_map(docs, stemDocument, "french")
# docs[1]$content
# 
# dtmTf <- DocumentTermMatrix(docs)
# dtmTf
# dtmTf <- NULL
# 
# # FIXME : 
# stemCompletion('homm',dictionary = nonStemmedDocs, c("first"))  #remarque le stemming fait perdre la lisibilite, on peut la retrouver avec stemCompletion => Renvoie hommage ?? Et femm renvoie femme mais pas femmes ??


# TODO : voir s'il faut éliminer les mots de moins de 2/3 lettres
```

Les résultats ne sont pas probants : homm renvoie hommage et homme par exemple. On abandonne cette étape.


```{r réduction des séquences espaces}
docs <- tm_map(docs, stripWhitespace)
inspect(docs[1])

DocumentTermMatrix(docs)
```


## Statistiques descriptives
On souhaite désormais décrire un peu mieux notre DTM. Pour ce faire, en génère une nouvelle avec une pondération TF qui va nous permettre de calculer la distribution du nombre de mots dans les résumés, par collection, ...

On descend ainsi de plus de 100 mots par résumé (avant nettoyage), à environ 70 :

```{r distribution du nombre de mots dans les résumés}
dtmTf <- DocumentTermMatrix(docs, control=list(weighting=weightTf))
dtmTf_m <- as.matrix(dtmTf)
rowSums <- rowSums(dtmTf_m)
hist(rowSums,breaks=25, main="histogramme des nombres de mots par résumé (épuré)",
        xlab = "nombre de mots du résumé", ylab = "nombre de livres")
abline(v=mean(rowSums), col="red",lwd = 2)
legend(90, 700, legend="moyenne tous livres confondus",
       col="red", lty=1, cex=0.8, lwd = 2)


```

Intéressons nous ensuite à la distribution du nombre de mots par collection :

```{r distribution du nombre de mots par collection}
# FIXME : on devrait faire le plot / boxplot sur la DTM et non le CSV de départ.
summary(rowSums)
plot(rowSums, col = dfBookSummaries$collection) # Pas très utile
abline(h=mean(rowSums), col = "red", lwd = 2)

boxplot(rowSums~dfBookSummaries$collection, col = palette(), border="brown")

```


Puis aux mots les plus fréquents par collection :

```{r mots fréquents par collection}
collections <- levels(dfBookSummaries$collection)

dtmTf_m <- as.matrix(dtmTf)

# TODO : 
for (i in 1:length(collections)) {
  m <- dtmTf_m[dfBookSummaries$collection == collections[i],]
  wordsFreq <- sort(colSums(m),decreasing = TRUE)
  wordsFreq <- data.frame(word = names(wordsFreq), Freq = as.vector(wordsFreq)) # mise au format pour wordcloud2
  print(wordcloud2(data = wordsFreq[1:ncol(dtmTf_m),]))
}

# TODO : peut-être faut-il supprimer les mots qui apparaissent sur toutes les collections ? Par exemple "vie"
# Requêtes SQL : 
```



## Réduction de la dimension
Bien que nous ayons normalisé le texte, retiré les stopwords et tous les caractères parasites, notre matrice possède plusieurs dizaines de milliers de colonnes.

On a déjà retiré à la main les mots très fréquents et qui n'améliorent pas la compréhension du résumé : auteur, nouvelle, livre.

On expérimente dans cette section 2 autres techniques : réduction du vocabulaire trop sparse ; variables avec peu de variance.

### Réduction du vocabulaire trop sparse

Au lieu de prendre les mots apparaissant au moins une fois, on prend le nombre de mots apparaissant au moins 2 fois afin de limiter un peu le nombre de colonnes avant même de calculer la matrice. Ce nombre est choisi arbitrairement : le vocabulaire du corpus représentant 30k mots sur 3k articles, 2 semble légitimement faible.

Au moment de créer la matrice, on en profite aussi pour calculer les pondérations `tfidf` qui sont des pondérations classiques pour le texte (pondère beaucoup les mots qui apparaissent souvent dans un document mais rarement dans le corpus pris globalement, a particulièrement du sens pour les documents longs, pas pour des tweets typiquement). Les résumés de livres se situant entre les 2 extrêmes, nous avons choisi TF-IDF car ???? TODO

```{r réduction en conservant les mots plus fréquents}
dtmTf <- DocumentTermMatrix(docs, control=list(weighting=weightTf))
minfreq <- findFreqTerms(dtmTf, 2) # On fonctionne ici toujours sur la DTM pondéré par TF
dtmTf <- DocumentTermMatrix(docs, control=list(dictionary = minfreq, weighting=weightTf))
dtmTfIdf <- DocumentTermMatrix(docs, control=list(dictionary = minfreq, weighting=weightTfIdf))
dtmTfIdf
dtmTf
```

Grâce à ce paramètre on descend à un nombre de variables plus gérable (16k vs. 30k). On a donc supprimé 15k variables qui apparaissaient une seule fois dans tout le corpus.

La sparsity a légèrement diminué. En revanche le nombre d'entrée non vides a aussi diminué, en passant de 160k à 147k.

Remarque : les termes présents au moins 2 fois pourraient l'être aussi sur n collections...ou sur un seul document.

La fonction `removeSparseTerms` du package `tm` propose une approche similaire bien que plus boîte noire (ici on ne filtre pas de cette manière, le code est laissé à titre informatif) :

```{r réduction de la dimension par suppression des termes sparse}
#dtmTfIdf2 <- dtmTfIdf
#dtmTf2 <- dtmTf

dtmTfIdf <- removeSparseTerms(dtmTfIdf, 0.97)
dtmTfIdf
dtmTf <- removeSparseTerms(dtmTf, 0.97)
dtmTf

dtmTf_m <- as.matrix(dtmTf)
dtmTfIdf_m <- as.matrix(dtmTfIdf)

colnames(dtmTfIdf_m)
```

On retrouve un vocabulaire propre à certaines collections : imaginaire (planete, aventure, univers, roi, ...), thriller (victime, polar, tueur, cadavre, ...), classique (vie, mort, maison, famille, ...)

### Suppression des variables avec peu de variance
En complément, on peut appliquer une méthode qui va identifier et supprimer les variables ayant très peu de variance (cad. qui apportent très peu d'informations) :

```{r préparation des matrices TF et TF-IDF}
# Faut-il le faire sur la dtm Tf ou TF-IDF ? On fait les 2...
dtmTf_m <- as.matrix(dtmTf)
dtmTfIdf_m <- as.matrix(dtmTfIdf)

dim(dtmTf_m)
dim(dtmTfIdf_m)
```

On lance ensuite le traitement NZV :
```{r réduction par suppression des variables avec peu de variance}
library(caret)

nzv <- nearZeroVar(dtmTf_m)
dtmTf_m_nzv <- dtmTf_m[, -nzv]

dim(dtmTf_m_nzv)
colnames(dtmTf_m_nzv)

nzvTfIdf <- nearZeroVar(dtmTfIdf_m)
dtmTfIdf_m_nzv <- dtmTfIdf_m[, -nzv]

dim(dtmTfIdf_m_nzv)
colnames(dtmTfIdf_m_nzv)

```

Cette fonction conserverait seulement 43 variables sur les 30k du départ, que ce soit sur la matrice TF ou TF-IDF. On ne va pas utiliser cette méthode car intuitivement on perd énormément de mots identifiés à l'étape précédente.


## Nuages de mots
A présent que nous disposons d'une matrice relativement propre, nous allons générer un nuage de mots par collection pour tenter de faire émerger quelques thèmes :

```{r nuages de mots par collection}
collections <- levels(dfBookSummaries$collection)

dtmTf_m <- as.matrix(dtmTf)
dim(dtmTf_m)
dim(dfBookSummaries)
ncol(dtmTf_m)

for (i in 1:length(collections)) {
  m <- dtmTfIdf_m[dfBookSummaries$collection == collections[i],]
  wordsFreq <- sort(colSums(m),decreasing = TRUE)
  wordsFreq <- data.frame(word = names(wordsFreq), Freq = as.vector(wordsFreq)) # mise au format pour wordcloud2
  print(wordcloud2(data = wordsFreq[1:ncol(dtmTf_m),]))
  }
```


## Classification non supervisée : ACP / SKmeans / LDA

### ACP
On tente ensuite une ACP sur la matrice :

Voir : https://www.usna.edu/Users/cs/lmcdowel/pubs/UnderhillIRI2007Final.pdf
Et :
https://www.kdnuggets.com/2015/05/7-methods-data-dimensionality-reduction.html

```{r acp}
# install.packages("FactoMineR")
library(FactoMineR)
PCAResult <- PCA(dtmTf_m, scale.unit = FALSE, ncp = 5)
PCAResult$var$cos2
```

On voit un axe porté par "Vie" et l'autre par "Monde" : roman réaliste versus. imaginaire ? Pas pertinent, on obtient du 3%


### SKmeans

```{r skmeans}
# install.packages("skmeans")
library(skmeans)

## On partitionne en 5 clusters.
rowTotals <- apply(dtmTf_m , 1, sum) # calcule la somme des termes dans chaque document
nonEmptyDtmTf_m   <- dtmTf_m[rowTotals> 0, ] # retire les documents vides, il peut arriver après nettoyage de se retrouver avec des documents vides, par exemple si on travaille sur des documents très courts type tweet

dim(nonEmptyDtmTf_m)

sk <- skmeans(x = nonEmptyDtmTf_m, 4)
## On regarde la répartition dans les clusters
table(sk$cluster)/sum(table(sk$cluster))

nonEmptyDfBookSummaries <- dfBookSummaries[-which(rowTotals == 0),]
nonEmptyDtmTf <- dtmTf[-which(rowTotals == 0),]
table(nonEmptyDfBookSummaries$collection, sk$cluster)


```


```{r kmeans}
### Normalize the vectors so Euclidean makes sense with kmeans
norm_eucl <- function(m) m/apply(m, MARGIN=1, FUN=function(x) sum(x^2)^.5)
m_norm <- norm_eucl(as.matrix(nonEmptyDtmTf_m))


set.seed(123)

# Finding the optimal number of clusters

# Compute the WSS for 
# k = 2 to k.max (kmeans)
k.max <- 15
sil2 <- (nrow(m_norm)-1)*sum(apply(m_norm,2,var))
for(i in 2:k.max){
  set.seed(123)
  sil2[i] <- sum(kmeans(m_norm, centers = i, iter.max = 20)$withinss)
  #ss2 <- silhouette(k.res2$cluster, dist(m_norm))
  #sil2[i] <- mean(ss2[, 3])
}

# Plot the elbow (kmeans)
plot(1:k.max, sil2, type = "b", pch = 19, 
     frame = FALSE, xlab = "Number of clusters k")
#abline(v = which.min(sil2), lty = 2)


# cluster into x (optimal) clusters - kmeans
set.seed(123)
cl2 <- kmeans(m_norm, 4, iter.max = 20)
table(cl2$cluster)
# show clusters using the first 2 principal components (ACP)

acp <- prcomp(m_norm)
acp
plot(acp$x, col=cl2$cluster)


nonEmptyDfBookSummaries <- dfBookSummaries[-which(rowTotals == 0),]
table(nonEmptyDfBookSummaries$collection, cl2$cluster)
```

On note 2 décrochages : à 4 et à 10. La découpe à 4 sépare le mieux les groupes.



### Topic modeling
On va regarder à présent les résultats d'une LDA, l'intérêt c'est que l'on regroupe les documents et les mots simultanément cette fois et que cela permet une exploration de résultats beaucoup plus aisée. La première chose à faire est de vérifier que l'on a pas de documents sans mot (notamment après avoir filtré sur la sparsité/parcimonie).

!!! La LDA s'applique sur une matrice documents termes sans pondération (nombre d'occurrences) !!! :
```{r topic modeling}

library(topicmodels)
# Paramètres du Gibbs sampling
burnin <- 500
iter <- 500
thin <- 100
seed <- list(2003,5,63,100001,765) # arbitraire
nstart <- 5
best <- TRUE

# Nombre de thèmes
k <- 10 # arbitraire à ce stade, nécessiterait d'être optimisé

# Estimation de la LDA
ldaOut <-LDA(nonEmptyDtmTf_m, k, method="Gibbs", control = list(nstart = nstart, seed = seed, best = best, burnin = burnin, iter = iter, thin = thin))

ldaOut.topics <- as.matrix(topics(ldaOut))
table(ldaOut.topics)

ldaOut.terms <- as.matrix(terms(ldaOut,5))
ldaOut.terms
```

