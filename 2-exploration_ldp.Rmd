---
title: "Exploration Livre de Poche"
author: "Jérôme THIBAULT"
date: "25/04/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Chargement des packages

```{r chargement des packages}

# Installer
# install.packages("tm")  # pour le text mining
# install.packages("SnowballC") # pour le text stemming
# install.packages("wordcloud") # générateur de word-cloud
# install.packages("RColorBrewer") # Palettes de couleurs
library(tidyverse)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
```


## Import et première description du jeu de données

```{r description sommaire}
dfBookSummaries <- read_csv("dfAllBooksSummaries2405.csv")
dfBookSummaries$X1 <- NULL
dfBookSummaries$X1_1 <- NULL

library(ggplot2)
table(dfBookSummaries$genre)
table(dfBookSummaries$genre) / sum(table(dfBookSummaries$genre))
ggplot(data = dfBookSummaries) + geom_bar(mapping = aes(x = genre, colour = genre))

# Compter le nombre de doublons
length(which(duplicated(dfBookSummaries$text) == TRUE))
dfBookSummaries[which(duplicated(dfBookSummaries$text)),]

library(dplyr)

# 48 doublons de résumés trouvés, intra-collection (bcp de résumés vides car livres pas encore parus)
# On les supprime
dfBookSummaries <- dfBookSummaries %>% distinct()
```



```{r corpus et dtm}
docs <- Corpus(VectorSource(dfBookSummaries$text))

inspect(docs[1])

# Suppression des accents : OK
library(stringi)
library(stringr)

nfdRemoveAccents <- function(x) {
  nfdX <- stri_trans_nfd(x)
  return(stri_replace_all_regex(nfdX, "\\p{Mn}+", ""))
}
docs <- tm_map(docs, content_transformer(nfdRemoveAccents))

# Convertir le texte en minuscule : OK
docs <- tm_map(docs, content_transformer(tolower))

# Remplacer toute la ponctuation par des espaces : OK
# FIXME : attention les stopwords pourraient ne plus fonctionner (l' remplacé par l)
removePonctuation <- function(x) {
  return(stri_replace_all_regex(x, "\\p{P}+", " "))
}
docs <- tm_map(docs, content_transformer(removePonctuation))

# Supprimer le texte en gras récupéré par erreur par le scraper (et difficile à automatiser)
# docs <- tm_map(docs, toSpace, "collection dirigée par gérard klein")

# Supprimer les nombres
docs <- tm_map(docs, removeNumbers)
# Supprimer une liste de mots non supportés par le dictionnaire de stop words fr
docs <- tm_map(docs, removeWords, c("plus", "les", "d", "l", "c", "d", "resume", "science-fiction", "ou", "plus", "tome", "serie", "roman", "romans", "collection", "livre", "gerard", "dirigee", "prix", "king", "auteur", "stephen"))

# Supprimer les mots outils français
 # FIXME : corriger les problèmes d'accent
# mySW = read.csv("jltStopwords.txt", sep = "", stringsAsFactors = FALSE, header = FALSE)
# docs <- tm_map(docs, removeWords, mySW$V1)

docs <- tm_map(docs, removeWords, stopwords("french"))
# Supprimer les ponctuations
docs <- tm_map(docs, removePunctuation)
# Supprimer les espaces vides supplémentaires
docs <- tm_map(docs, stripWhitespace)


# FIXME : Text stemming. Donne de drôles de résultats !!
# docs <- tm_map(docs, stemDocument)


# TODO : voir s'il faut éliminer les mots de moins de 3 lettres


dtm <- DocumentTermMatrix(docs)
dtm
# <<DocumentTermMatrix (documents: 5048, terms: 39573)>>
# Non-/sparse entries: 347222/199417282
# Sparsity           : 100%
# Maximal term length: 30
# Weighting          : term frequency (tf)

# distribution de la longueur d'un terme, distribution du nombre de mots par document
dtmMatrix <- as.matrix(dtm) # renvoie pour chaque document/terme, le nombre de fois
rowSums <- rowSums(dtmMatrix)

# FIXME : Erreur : ggplot2 doesn't know how to deal with data of class numeric
# ggplot(data = rowSums) + geom_bar(mapping = aes(x = dfBookSummaries$genre, colour = as.factor(dfBookSummaries$genre)))

# 12 résumés presque vides, à supprimer
dfBookSummaries <- dfBookSummaries[which(rowSums>=10),]
docs <- Corpus(VectorSource(dfBookSummaries$text))
dtm <- DocumentTermMatrix(docs)
dtmMatrix <- as.matrix(dtm)
rowSums <- rowSums(dtmMatrix)

# Distribution du nombre de mots par genre
summary(rowSums)

plot(rowSums, col = as.factor(dfBookSummaries$genre))
abline(h=median(rowSums), col = "green")
hist(rowSums, breaks = 100)
boxplot(rowSums~dfBookSummaries$genre, col = as.factor(dfBookSummaries$genre))

dtmMatrix <- NULL

# TODO : Est-ce que dans le résumé, on trouve le nom du genre ? Tf-IDF donnera moins d'importance à "Science" s'il est très répandu dans tout le corpus.


```


## Nettoyage des données

```{r nettoyage}



# On vectorise le corpus après nettoyage
dtm <- DocumentTermMatrix(docs)
```


## Description plus complète après nettoyage

```{r contrôle descriptif}
dfDTM <- as.data.frame(as.matrix(dtm))



# TODO : récupérer les couvertures ?
# TODO : topic modeling pour adresser les synonymes ?



minfreq <- findFreqTerms(dtm, 10)
dtm <- DocumentTermMatrix(docs, control=list(dictionary = minfreq, weight=weightTfIdf))
dtmCleanMatrix <- as.data.frame(as.matrix(dtm))

# TODO : on pourrait encore supprimer quelques mots (en regardant les noms des colonnes de la DTM). Voir s'ils sont associés souvent à d'autres mots ? Type saga la plus vendue
# Normalement on peut faire cette manip, car n'influence pas le TF-IDF :
# dtmCleanMatrix$maigret <- NULL
# dtmCleanMatrix$nouvelle <- NULL


library(wordcloud2)

categories <- levels(as.factor(dfBookSummaries$genre))
for (i in 1:length(categories)){
  m <- dtmCleanMatrix[dfBookSummaries$genre == categories[i],]
  wordsFreq <- sort(colSums(m),decreasing = TRUE)
  wordsFreq <- data.frame(word = names(wordsFreq), Freq = as.vector(wordsFreq)) # mise au format pour wordcloud2
  print(wordcloud2(data = wordsFreq[1:500,],minSize = 1, size = 3))
}


# Après nettoyage, on descend de plus de 100 mots par résumé à moins de 40
rowSums <- rowSums(dtmCleanMatrix)
plot(rowSums, col = as.factor(dfBookSummaries$genre))
abline(h=median(rowSums), col = "green")
hist(rowSums, breaks = 100)


# Similarité entre mots
# TODO : findAssocs(dtm, terms = "monde", corlimit = 0.3)

# TODO : Word2Vec : mais pour faire quoi exactement dans notre cas ?

# TODO : Classification non supervisée : SKmeans / CAH / LDA
```


## Classification non supervisée LDA (Topic Modeling)

```{r topic modeling}
rowTotals <- apply(dtmCleanMatrix , 1, sum) # calcule la somme des termes dans chaque document
dtmCleanMatrix   <- dtmCleanMatrix[rowTotals> 0, ] # retire les documents vides, il peut arriver après nettoyage de se retrouver avec des documents vides, par exemple si on travaille sur des documents très courts type tweet

# install.packages("topicmodels")
library(topicmodels)
# Paramètres du Gibbs sampling
burnin <- 500
iter <- 500
thin <- 100
seed <- list(2003,5,63,100001,765) # arbitraire
nstart <- 5
best <- TRUE

# Nombre de thèmes
k <- 2 # arbitraire à ce stade, nécessiterait d'être optimisé

# Estimation de la LDA
ldaOut <-LDA(dtmCleanMatrix,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))


ldaOut.topics <- as.matrix(topics(ldaOut))
table(ldaOut.topics)

ldaOut.terms <- as.matrix(terms(ldaOut,10))
ldaOut.terms

topicProbabilities <- as.data.frame(ldaOut@gamma)
names(topicProbabilities) <- apply(data.frame(ldaOut.terms),2,function(x) paste(x,collapse=' '))
topicProbabilities[1:100,]  #pour les dix premiers documents arbitrairement, on voit que la plupart du temps un topic est prépondérant.

# TODO : tester la précision des catégories vs. les Y de sortie du premier dataframe (for JL : "accuracy").
# TODO : kmeans et CAH (sur matrice nettoyée)
```

