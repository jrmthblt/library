---
title: "Exploration"
author: "Jérôme THIBAULT"
date: "25/04/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r chargement des packages}
library(rvest)
library(tidyverse)
library(testthat)
# Installer
# install.packages("tm")  # pour le text mining
# install.packages("SnowballC") # pour le text stemming
# install.packages("wordcloud") # générateur de word-cloud
# install.packages("RColorBrewer") # Palettes de couleurs
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
```

```{r description sommaire}
dfBookSummaries <- read_csv("dfBookSummaries.csv")
library(ggplot2)
table(dfBookSummaries$genre)
table(dfBookSummaries$genre)/sum(table(dfBookSummaries$genre))
ggplot(data = dfBookSummaries) + geom_bar(mapping = aes(x = genre, colour = genre))


docs <- Corpus(VectorSource(dfBookSummaries$text))
# Nombre de documents, nombre de termes
dtm <- DocumentTermMatrix(docs)
inspect(dtm)
# <<DocumentTermMatrix (documents: 320, terms: 8006)>>
# Non-/sparse entries: 20578/2541342
# Sparsity           : 99%
# Maximal term length: 26
# Weighting          : term frequency (tf)


# distribution de la longueur d'un terme, distribution du nombre de mots par document
dtmMatrix <- as.matrix(dtm) # renvoie pour chaque document/terme, le nombre de fois
rowSums <- rowSums(dtmMatrix)

ggplot(data = rowSums) + geom_bar(mapping = aes(x = genre, colour = genre))


# 2 résumés "vides" :
summary(rowSums)
tapply(rowSums, dfBookSummaries$genre, min)
tapply(rowSums, dfBookSummaries$genre, mean)
tapply(rowSums, dfBookSummaries$genre, max)
tapply(rowSums, dfBookSummaries$genre, sd)
# barplot(tapply(rowSums, dfBookSummaries$genre, mean))
# tapply(rowSums, dfBookSummaries$genre, function(x) {list(mean(x), sd(x), max(x))})
plot(rowSums, col = as.factor(dfBookSummaries$genre))
hist(rowSums)
dfBookSummaries[which(rowSums<10),2]

findMostFreqTerms(dtm, n = 30L, INDEX = rep(1 : 2, each = 160L))

# Est-ce que dans le résumé, on trouve le nom du genre ? Tf-IDF donnera moins d'importance à "Science" s'il est très répandu dans tout le corpus.

plot(colSums(dtmFantastique))

```


```{r nettoyage}
# TODO : 1 livre n'a pas de résumé (L'anniversaire du monde) . 1 doublon de résumé détecté


# Convertir le texte en minuscule
docs <- tm_map(docs, content_transformer(tolower))
# Remplacer les points par des espaces
toSpace <- content_transformer(function (x , pattern) { gsub(pattern, " ", x)})
docs <- tm_map(docs, toSpace, "\\.")
# Remplacer les apostrophes par des espaces
docs <- tm_map(docs, toSpace, "'")
docs <- tm_map(docs, toSpace, "’")
docs <- tm_map(docs, toSpace, "«")
docs <- tm_map(docs, toSpace, "»")

toA <- content_transformer(function (x , pattern) { gsub(pattern, "a", x)})
docs <- tm_map(docs, toA, "à")
docs <- tm_map(docs, toA, "á")


toE <- content_transformer(function (x , pattern) { gsub(pattern, "e", x)})
docs <- tm_map(docs, toE, "é")
docs <- tm_map(docs, toE, "è")
docs <- tm_map(docs, toE, "ê")
docs <- tm_map(docs, toE, "ë")

toI <- content_transformer(function (x , pattern) { gsub(pattern, "i", x)})
docs <- tm_map(docs, toE, "ï")
docs <- tm_map(docs, toE, "î")

# Supprimer le texte en gras récupéré par erreur par le scraper (et difficile à automatiser)
docs <- tm_map(docs, toSpace, "collection dirigée par gérard klein")

# Supprimer les nombres
docs <- tm_map(docs, removeNumbers)
# Supprimer une liste de mots non supportés par le dictionnaire de stop words fr
docs <- tm_map(docs, removeWords, c("...", "d'", "l'", "resume", "science-fiction", "où", "plus", "tome", "serie", "roman", "romans", "collection", "livre", "gerard", "dirigee", "prix", "king", "auteur", "stephen"))
# Supprimer les mots outils français
mySW = read.csv("jltStopwords.txt", sep = "", stringsAsFactors = FALSE, header = FALSE) # TODO : corriger les problèmes d'accent
docs <- tm_map(docs, removeWords, mySW$V1)
# docs <- tm_map(docs, removeWords, stopwords("french"))
# Supprimer les ponctuations
docs <- tm_map(docs, removePunctuation)
# Supprimer les espaces vides supplémentaires
docs <- tm_map(docs, stripWhitespace)
# FIXME : Text stemming. Donne de drôles de résultats !!
# docs <- tm_map(docs, stemDocument)

# Suppression des accents
library(stringi)
library(stringr)
# accent <- function(x) stri_trans_general(x, "UTF8-ASCII") # cela signifie qu'on remplace un caractère encodé en Latin1 par son équivalent le plus proche en ASCII, il n'y a par exemple pas de caractères accentués en ASCII
# docs <- tm_map(docs, content_transformer(accent))

# TODO : voir s'il faut éliminer les mots de moins de 3 lettres
# TODO : récupérer les couvertures ?
# TODO : topic modeling pour adresser les synonymes ?

dtm <- DocumentTermMatrix(docs)
```

