---
title: "Scraping Livre de Poche website data"
output:
  html_document: default
  html_notebook: default
---

This R script scrapes data from Livre de Poche website. It retrieves all summaries of books of genre "Fantastique", "Fantasy" and "Science-Fiction".

First of all, we install the dependencies we need (the project is managed by Packrat).
```{r install dependencies}
# packrat::clean()
# install.packages("rvest")
# install.packages("testthat")
# install.packages("tidyverse")
```

Then we load them :
```{r}
library(rvest)
library(tidyverse)
# TODO : assert the title and summary
library(testthat)
```


Then, we try a simple scraping of one book to make sure it is properly working :
```{r one book scraping}

oneBook <- read_html("http://www.livredepoche.com/rainbows-end-vernor-vinge-9782253159933")

title <- oneBook %>% html_nodes(".book-title") %>% html_text()
summary <- oneBook %>% html_nodes(".view-content p") %>% html_text()

print(title)
print(summary)

#Now we ensure the proper retrieval of one book, we perform the same step for each book in a genre :

scraperDelay <- 1
rootUrl <- "http://www.livredepoche.com"


# Given a genre code and a page index, returns all books URLs of the current page
retrieveGenreCurrentPageBooksURLs <- function(rootUrl, pageIndex, genreCode) {
  pageNumberParameter <- "pageNumber"
  rootSearchUrl <- paste(rootUrl, "/collection-imaginaire?sort_by=field_book_publication_date&sort_order=DESC&page=", pageNumberParameter, "&f[0]=field_book_collection%3A10489&f[1]=field_tags%", sep="")
  genreCurrentPageURL <- paste(gsub(pageNumberParameter, as.character(pageIndex), rootSearchUrl), genreCode, sep="")
  genreCurrentPage <- read_html(genreCurrentPageURL)
  return(genreCurrentPage %>% html_nodes(".book-title a") %>% html_attr("href"))
}


# Takes an HTML node list and extracts the book summary
retrieveBookSummary <- function(book) {
  return(book %>% html_nodes(".book-resume-block") %>% html_text())
  # FIXME : le p n'est pas toujours présent après le block resume
  # return(book %>% html_nodes(".book-resume-block p") %>% html_text())
}


# Main Code
literaryGenres <- tribble(
  ~label, ~code,
  "Fantastique", "3A297",
  "Science-fiction", "3A7141"
)
```

```{r}
bookSummaries <- list()
```
Fantastique
```{r}
# Scraping "Fantastique" books
for (i in 0:15) {
  ftSummarie <- sapply(retrieveGenreCurrentPageBooksURLs(rootUrl, i, literaryGenres[1,2]), function(bookURL) {
    book <- read_html(paste(rootUrl, bookURL, sep=""),encoding = "UTF-8" )
    Sys.sleep(scraperDelay)
    return(retrieveBookSummary(book))
  })
  bookSummaries <- c(bookSummaries, ftSummarie)
}
```
SF
```{r}
# Scraping "Science-Fiction" books
for (i in 0:15) {
  sfSummaries <- sapply(retrieveGenreCurrentPageBooksURLs(rootUrl, i, literaryGenres[2,2]), function(bookURL) {
    book <- read_html(paste(rootUrl, bookURL, sep=""), encoding = "UTF-8")
    Sys.sleep(scraperDelay)
    return(retrieveBookSummary(book))
  })
  bookSummaries <- c(bookSummaries, sfSummaries)  
}


dfBookSummaries <- data_frame(line = 1:length(bookSummaries), text = unlist(bookSummaries),genre=c(rep("Fantastique",160),rep("Science-Fiction",160)))

# Write the result to ensure we will not need to scrap every time we run the project
write.csv(dfBookSummaries, file="books.csv")



